= Introduction

It's time for us to put our cluster to work and deploy a workload.
We're going to build an example Java application, https://github.com/redhat-mw-demos/microsweeper-quarkus/tree/ARO[microsweeper], using https://quarkus.io/[Quarkus] (a Kubernetes Native Java stack) and https://azure.microsoft.com/en-us/products/postgresql/[Azure Database for PostgreSQL].
We'll then deploy the application to our Azure Red Hat OpenShift cluster, connect to the database using Azure Private Link, and automate the deployment with OpenShift Pipelines.

== Create Azure Database for PostgreSQL instance

. First, let's create a namespace (also known as a project in OpenShift).
To do so, run the following command:
+
[source,sh,role=execute]
----
oc new-project microsweeper-ex
----
+
.Sample Output
[source,text,options=nowrap]
----
Now using project "microsweeper-ex" on server "https://api.c90qz1cy.eastus.aroapp.io:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname
----

. Create the Azure Postgres Server resource.
To do so, run the following command (this command will take ~ 5mins)
+
[source,sh,role=execute]
----
az postgres server create --resource-group openenv-${GUID} \
  --location eastus --sku-name GP_Gen5_2 \
  --name microsweeper-${GUID} --storage-size 51200 \
  --admin-user myAdmin --admin-pass %generated_password% \
  --public 0.0.0.0
----
+
.Sample Output
[source,text,options=nowrap]
----
Checking the existence of the resource group 'openenv-qsd7p'...
Resource group 'openenv-qsd7p' exists ? : True
Creating postgres Server 'microsweeper-qsd7p' in group 'openenv-qsd7p'...
Your server 'microsweeper-qsd7p' is using sku 'GP_Gen5_2' (Paid Tier). Please refer to https://aka.ms/postgres-pricing  for pricing details
Configuring server firewall rule, 'azure-access', to accept connections from all Azure resources...
Make a note of your password. If you forget, you would have to reset your password with 'az postgres server update -n microsweeper-qsd7p -g openenv-qsd7p -p <new-password>'.
{
  "additionalProperties": {},
  "administratorLogin": "myAdmin",

[...Output omitted...]
----
+
[NOTE]
====
For the sake of the workshop we are creating a public database that any host in Azure can connect to.
In a real world scenario you would create a private database and connect to it over a private link service
====

. Make sure the PostgreSQL client software is installed on your bastion VM:
+
[source,sh,role=execute]
----
dnf -y install postgresql
----

. Check connectivity from to the database.
To do so, run the following command:
+
[source,sh,role=execute]
----
psql \
  "host=microsweeper-${GUID}.postgres.database.azure.com port=5432
  dbname=postgres
  user=myAdmin@microsweeper-${GUID}.postgres.database.azure.com password=%generated_password% sslmode=require" \
  -c "select now();"
----
+
.Sample Output
[source,text,options=nowrap]
----
-------------------------------
 2023-04-24 20:44:38.088556+00
(1 row)
----

== Build and deploy the Microsweeper app

Now that we've got an Azure managed PostgreSQL instance up and running, let's build and deploy our application.

. In order to build the application you will need the Java JDK 17 and the Quarkus CLI installed. Java JDK 17 is already installed on your bastion VM so let's install the Quarkus CLI:
+
[source,sh,role=execute]
----
curl -Ls https://sh.jbang.dev | bash -s - trust add https://repo1.maven.org/maven2/io/quarkus/quarkus-cli/

curl -Ls https://sh.jbang.dev | bash -s - app install --fresh --force quarkus@quarkusio

source $HOME/.bashrc
----

. Double check the Quarkus CLI version:
+
[source,sh,role=execute]
----
quarkus --version
----
+
.Sample Output
[source,text,options=nowrap]
----
3.1.0.Final
----

. Now, let's clone the application from GitHub to our local bastion VM.
To do so, run the following command:
+
[source,sh,role=execute]
----
cd $HOME

git clone https://github.com/rh-mobb/aro-workshop-app.git
----
+
.Sample Output
[source,text,options=nowrap]
----
Cloning into 'aro-workshop-app'...
remote: Enumerating objects: 152, done.
remote: Counting objects: 100% (16/16), done.
remote: Compressing objects: 100% (14/14), done.
remote: Total 152 (delta 12), reused 2 (delta 2), pack-reused 136
Receiving objects: 100% (152/152), 6.57 MiB | 31.29 MiB/s, done.
Resolving deltas: 100% (55/55), done.
----

. Next, let's change directory into the newly cloned Git repository.
To do so, run the following command:
+
[source,sh,role=execute]
----
cd $HOME/aro-workshop-app
----

. Next, we will add the OpenShift extension to the Quarkus CLI.
To do so, run the following command:
+
[source,sh,role=execute]
----
quarkus ext add openshift
----
+
.Sample Output
[source,text,options=nowrap]
----
Looking for the newly published extensions in registry.quarkus.io
[SUCCESS] ✅  Extension io.quarkus:quarkus-openshift has been installed
----

. We also want Quarkus to be able to use OpenShift ConfigMaps and Secrets:
+
[source,sh,role=execute]
----
quarkus ext add kubernetes-config
----
+
.Sample Output
[source,text,options=nowrap]
----
Looking for the newly published extensions in registry.quarkus.io
[SUCCESS] ✅  Extension io.quarkus:quarkus-openshift has been installed
----

. Create a OpenShift secret containing Database credentials for Quarkus to use:
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: v1
kind: Secret
metadata:
  name: microsweeper-secret
  namespace: microsweeper-ex
type: Opaque
stringData:
  PG_URL: jdbc:postgresql://microsweeper-${GUID}.postgres.database.azure.com:5432/postgres
  PG_USER: myAdmin@microsweeper-${GUID}.postgres.database.azure.com
  PG_PASS: %generated_password%
EOF
----
+
.Sample Output
[source,text,options=nowrap]
----
secret/microsweeper-secret created
----

. Now, we'll configure Quarkus to use the PostgreSQL database that we created earlier in this section.
To do so, we'll create an `application.properties` file using by running the following command:
+
[source,sh,role=execute]
----
cat <<"EOF" > $HOME/aro-workshop-app/src/main/resources/application.properties
# Database configurations
%prod.quarkus.datasource.db-kind=postgresql
%prod.quarkus.datasource.jdbc.url=${PG_URL}
%prod.quarkus.datasource.username=${PG_USER}
%prod.quarkus.datasource.password=${PG_PASS}
%prod.quarkus.datasource.jdbc.driver=org.postgresql.Driver
%prod.quarkus.hibernate-orm.database.generation=drop-and-create
%prod.quarkus.hibernate-orm.database.generation=update

# OpenShift configurations
%prod.quarkus.kubernetes-client.trust-certs=true
%prod.quarkus.kubernetes.deploy=true
%prod.quarkus.kubernetes.deployment-target=openshift
%prod.quarkus.openshift.build-strategy=docker
%prod.quarkus.openshift.expose=true
%prod.quarkus.openshift.deployment-kind=Deployment
%prod.quarkus.container-image.group=microsweeper-ex
%prod.quarkus.openshift.env.secrets=microsweeper-secret
EOF
----

. Now that we've provided the proper configuration, we will build our application.
We'll do this using https://github.com/openshift/source-to-image[source-to-image], a tool built-in to OpenShift.
To start the build and deploy, run the following command:
+
[source,sh,role=execute]
----
quarkus build --no-tests
----
+
.Sample Output
[source,text,options=nowrap]
----
[INFO] Scanning for projects...
Downloading from central: https://repo.maven.apache.org/maven2/io/quarkus/platform/quarkus-maven-plugin/2.11.2.Final/quarkus-maven-plugin-2.11.2.Final.pom
Downloaded from central: https://repo.maven.apache.org/maven2/io/quarkus/platform/quarkus-maven-plugin/2.11.2.Final/quarkus-maven-plugin-2.11.2.Final.pom (6.9 kB at 11 kB/s)

[...Output omitted...]

[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  02:41 min
[INFO] Finished at: 2023-04-24T20:58:59Z
[INFO] ------------------------------------------------------------------------
----
+
[INFO]
====
Quarkus will build the .jar locally and then work with the OpenShift build system to inject it into a Red Hat UBI image, save that to the inbuild OpenShift registry, and then run the resulting image in OpenShift.
====

. We want to see custom metrics from the Quarkus app (they're exposed by the Quarkus micrometer plugin) so we can configure a Prometheus `ServiceMonitor` resource to watch for the applications label.
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: microsweeper-monitor
  name: microsweeper-monitor
  namespace: microsweeper-ex
spec:
  endpoints:
  - interval: 30s
    targetPort: 8080
    path: /q/metrics
    scheme: http
  selector:
    matchLabels:
      app.kubernetes.io/name: microsweeper-appservice
EOF
----
+
.Sample Output
[source,text,options=nowrap]
----
servicemonitor.monitoring.coreos.com/microsweeper-monitor created
----

. Change back to your previous working directory
+
[source,sh,role=execute]
----
cd $HOME
----

== Review

Let's take a look at what this command did, along with everything that was created in your cluster.
Return to your tab with the OpenShift Web Console.

=== Container Images

From the Administrator perspective, expand _Builds_ and then _ImageStreams_, and select the _microsweeper-ex_ project.

image::../media/web-console-imagestreams.png[OpenShift Web Console - Imagestreams]

You will see two images that were created on your behalf when you ran the quarkus build command.
There is one image for `openjdk-11` that comes with OpenShift as a Universal Base Image (UBI) that the application will run under.
With UBI, you get highly optimized and secure container images that you can build your applications with.
For more information on UBI please read this https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image[article].

The second image you see is the the `microsweeper-appservice` image.
This is the image for the application that was built automatically for you and pushed to the built-in container registry inside of OpenShift.

=== Image Build

How did those images get built you ask?
Back on the OpenShift Web Console, click on _BuildConfigs_ and then the _microsweeper-appservice_ entry.

image::../media/web-console-buildconfigs.png[OpenShift Web Console - BuildConfigs]
image::../media/web-console-microsweeper-appservice-buildconfig.png[OpenShift Web Console - microsweeper-appservice BuildConfig]

When you ran the `quarkus build` command, this created the BuildConfig you can see here.
In our quarkus settings, we set the deployment strategy to build the image using Docker.
The Dockerfile file from the git repo that we cloned was used for this BuildConfig.

!!!
info "A build configuration describes a single build definition and a set of triggers for when a new build is created.
Build configurations are defined by a BuildConfig, which is a REST object that can be used in a POST to the API server to create a new instance."

You can read more about BuildConfigs https://docs.openshift.com/container-platform/latest/cicd/builds/understanding-buildconfigs.html[here]

Once the BuildConfig was created, the source-to-image process kicked off a Build of that BuildConfig.
The build is what actually does the work in building and deploying the image.
We started with defining what to be built with the BuildConfig and then actually did the work with the Build.
You can read more about Builds https://docs.openshift.com/container-platform/latest/cicd/builds/understanding-image-builds.html[here]

To look at what the build actually did, click on Builds tab and then into the first Build in the list.

image::../media/web-console-builds.png[OpenShift Web Console - Builds]

On the next screen, explore around.
Look specifically at the YAML definition of the build and the logs to see what the build actually did.
If you build failed for some reason, the logs are a great first place to start to look at to debug what happened.

image::../media/web-console-build-logs.png[OpenShift Web Console - Build Logs]

=== Image Deployment

After the image was built, the source-to-image process then deployed the application for us.
In the quarkus properties file, we specified that a deployment should be created.
You can view the deployment under _Workloads_ \-> _Deployments_, and then click on the Deployment name.

image::../media/web-console-deployments.png[OpenShift Web Console - Deployments]

Explore around the deployment screen, check out the different tabs, look at the YAML that was created.

image::../media/web-console-deployment-yaml.png[OpenShift Web Console - Deployment YAML]

Look at the pod the deployment created, and see that it is running.

image::../media/web-console-deployment-pods.png[OpenShift Web Console - Deployment Pods]

The last thing we will look at is the route that was created for our application.
In the quarkus properties file, we specified that the application should be exposed to the Internet.
When you create a Route, you have the option to specify a hostname.
To start with, we will just use the default domain that comes with ARO (`useast.aroapp.io` in our case).
In next section, we will expose the same application to a custom domain leveraging Azure Front Door.

You can read more about routes https://docs.openshift.com/container-platform/latest/networking/routes/route-configuration.html[in the Red Hat documentation]

From the OpenShift Web Console menu, click on _Networking_\->__Routes__, and the _microsweeper-appservice_ route.
image::../media/web-console-routes.png[OpenShift Web Console - Routes]

=== Test the application

While in the route section of the OpenShift Web Console, click the URL under _Location_: image::../media/web-console-route-link.png[OpenShift Web Console - Route Link]

You can also get the the URL for your application using the command line:

[source,sh,role=execute]
----
oc -n microsweeper-ex get route microsweeper-appservice -o jsonpath='{.spec.host}' ; echo
----

.Sample Output
[source,text,options=nowrap]
----
microsweeper-appservice-microsweeper-ex.apps.c90qz1cy.eastus.aroapp.io
----

=== View custom metrics for the App

Switch the OpenShift Web Console to the Developer view, select the project `microsweeper-ex` and go to menu:Observe[Metrics] and type `process_uptime_seconds` into custom metrics.
Switch the timeframe to `5min`.

[INFO]
====
While you're here, you might also want to look at the Dashboard tab to see the Project's CPU/Memory usage.
====

=== Application IP

Let's take a quick look at what IP the application resolves to.
Back in your terminal environment, run the following command:

[source,sh,role=execute]
----
nslookup $(oc -n microsweeper-ex get route microsweeper-appservice -o jsonpath='{.spec.host}')
----

.Sample Output
[source,text,options=nowrap]
----
Server:		168.63.129.16
Address:	168.63.129.16#53

Non-authoritative answer:
Name:	microsweeper-appservice-microsweeper-ex.apps.c90qz1cy.eastus.aroapp.io
Address: 40.88.34.241
----

Notice the IP address;
Can you guess where it comes from?

It comes from the ARO Load Balancer.
In this workshop, we are using a public cluster which means the load balancer is exposed to the Internet.
If this was a private cluster, you would have to have connectivity to the vNet ARO is running on.
This could be via a VPN connection, Azure ExpressRoute, or something else.

To view the ARO load balancer, on the Azure Portal, search for "Load Balancers" in the search bar and click on the _Load balancers_ service.

image::../media/azure-portal-load-balancer-menu.png[Azure Portal - Load Balancer Search]

You will notice two load balancers, one that has -internal in the name and one that does not.
The `-internal` load balancer is used for the OpenShift API.
The other load balancer (without the `-internal` suffix) in the name is use the public load balancer used for the default Ingress Controller.
Click into the load balancer for applications.

image::../media/azure-portal-load-balancer-list.png[Azure Portal - Load Balancer List]

On the next screen, click on Frontend IP configuration.
Notice the IP address of the 2nd load balancer on the list.
This IP address matches what you found with the nslookup command.

image::../media/azure-portal-lb-frontend-ip-config.png[Azure Portal - Load Balancer Frontend IP configuration]

For the fun of it, we can also look at what backends this load balancer is connected to.

image::../media/azure-portal-lb-frontend-ip-drilldown.png[Azure Portal - Load Balancer Frontend IP configuration]

Next, click on the pool that ends in 443.

image::../media/azure-portal-lb-frontend-ip-detail.png[Azure Portal - Load Balancer Frontend IP configuration]

Notice the _Backend pool_.
This is the subnet that contains all the worker nodes.
And the best part is all of this came with Azure Red Hat OpenShift out of the box!

image::../media/azure-portal-lb-backend-pool.png[Azure Portal - Load Balancer Backend Pool]
